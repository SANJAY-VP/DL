{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "__Importing__"
      ],
      "metadata": {
        "id": "0PMfEOEi3oIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVu_ymqBkDty",
        "outputId": "ccc71cd5-db08-44db-c389-894d9bb7003c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Data Preparation__"
      ],
      "metadata": {
        "id": "0j-6Y0hH3fqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # single channel\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_data, val_data = random_split(train_dataset, [50000, 10000])\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5,), (0.5,))\n",
        "# ])\n",
        "\n",
        "# train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# train_data, val_data = random_split(train_dataset, [50000, 10000])\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((32,32)),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomCrop(32, padding=4),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])\n",
        "\n",
        "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# train_data, val_data = random_split(train_dataset, [40000, 10000])\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])\n",
        "\n",
        "# train_dataset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "# test_dataset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "\n",
        "# train_data, val_data = random_split(train_dataset, [60000, 13257])  # SVHN has ~73k train samples\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((96, 96)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])\n",
        "\n",
        "# train_dataset = torchvision.datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
        "# test_dataset = torchvision.datasets.STL10(root='./data', split='test', download=True, transform=transform)\n",
        "\n",
        "# train_data, val_data = random_split(train_dataset, [4000, 1000])\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytnNRO0JEadI",
        "outputId": "2f8222a8-3388-415d-cf61-b6cbe2ee03f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 50000, Val: 10000, Test: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Model__"
      ],
      "metadata": {
        "id": "4h-6gXZz4Oul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  # conv_channels means no. of filters\n",
        "    def __init__(self, in_channels = 3, num_classes=10, conv_channels=[64, 128, 256], dropout_p=0.5):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # --- Convolutional feature extractor ---\n",
        "        self.features = nn.Sequential(\n",
        "            # in_channels out_channels  filter_size\n",
        "            nn.Conv2d(in_channels, conv_channels[0], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(conv_channels[0]),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            # kernel stride\n",
        "            nn.MaxPool2d(2, 2),  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(conv_channels[0], conv_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(conv_channels[1]),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),  # 16 -> 8\n",
        "\n",
        "            nn.Conv2d(conv_channels[1], conv_channels[2], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(conv_channels[2]),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),   # 8 -> 4\n",
        "\n",
        "            nn.Conv2d(conv_channels[2], 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # --- Fully connected classifier ---\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(conv_channels[-1], 256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xq-JoPhInw5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # resets gradients from the previous batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        val_acc = evaluate_model(model, val_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    # no gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return 100 * correct / total\n"
      ],
      "metadata": {
        "id": "13ogp7jwn18j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "model = SimpleCNN(in_channels=1).to(device)  # For MNIST/FashionMNIST\n",
        "# model = SimpleCNN(in_channels=3).to(device)  # For CIFAR-10/STL10/SVHN\n",
        "\n",
        "print(model)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# training\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
        "print(\"training completed \\n\")\n",
        "# testing\n",
        "test_acc = evaluate_model(model, test_loader)\n",
        "print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu_I5FuDn332",
        "outputId": "94553369-a426-4f11-952d-158201908316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch [1/10] - Loss: 0.1719 | Train Acc: 95.01% | Val Acc: 95.29%\n",
            "Epoch [2/10] - Loss: 0.0381 | Train Acc: 98.87% | Val Acc: 98.25%\n",
            "Epoch [3/10] - Loss: 0.0249 | Train Acc: 99.26% | Val Acc: 98.86%\n",
            "Epoch [4/10] - Loss: 0.0175 | Train Acc: 99.47% | Val Acc: 98.95%\n",
            "Epoch [5/10] - Loss: 0.0118 | Train Acc: 99.63% | Val Acc: 99.04%\n",
            "Epoch [6/10] - Loss: 0.0088 | Train Acc: 99.74% | Val Acc: 98.90%\n",
            "Epoch [7/10] - Loss: 0.0069 | Train Acc: 99.81% | Val Acc: 99.11%\n",
            "Epoch [8/10] - Loss: 0.0040 | Train Acc: 99.89% | Val Acc: 99.20%\n",
            "Epoch [9/10] - Loss: 0.0033 | Train Acc: 99.91% | Val Acc: 98.94%\n",
            "Epoch [10/10] - Loss: 0.0030 | Train Acc: 99.93% | Val Acc: 99.15%\n",
            "training completed \n",
            "\n",
            "Final Test Accuracy: 99.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST / Fashion-MNIST → 28×28\n",
        "\n",
        "CIFAR-10 / SVHN → 32×32\n",
        "\n",
        "STL-10 → 96×96"
      ],
      "metadata": {
        "id": "sz90Z5ENIPHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimiser** SGD\n",
        "\n",
        "---\n",
        "\n",
        "**mnist**  Train Acc: 99.93% Test Accuracy: 99.39%\n",
        "\n",
        "---\n",
        "\n",
        "**fashion mnist**  Train Acc: 97.01% Test Accuracy: 89.94%\n",
        "\n",
        "---\n",
        "\n",
        "**cifar10**: Train Acc: 77.59% Test Accuracy: 75.22%\n",
        "\n",
        "---\n",
        "\n",
        "**svhn**  Train Acc: 96.32% Test Accuracy: 91.29%\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**stl** Train Acc: 56.38  Test Accuracy: 46.46%\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**activation**: LeakyReLU\n",
        "\n",
        "---\n",
        "\n",
        "**mnist**  Train Acc: 99.70% Test Accuracy: 99.05%\n",
        "\n",
        "---\n",
        "\n",
        "**fashion mnist**  Train Acc: 97.64% Test Accuracy: 92.29%\n",
        "\n",
        "---\n",
        "\n",
        "**cifar10**: Train Acc: 81.25 Test Accuracy: 75.43%\n",
        "\n",
        "---\n",
        "\n",
        "**svhn**   Train Acc: 97.10%   Test Accuracy: 92.46%\n",
        "\n",
        "---\n",
        "\n",
        "**stl** Train Acc: 61.65  Test Accuracy: 50.00%\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**Extra Convolution Layer**\n",
        "\n",
        "---\n",
        "\n",
        "**mnist**  Train Acc: 99.77% Test Accuracy: 98.81%\n",
        "\n",
        "---\n",
        "\n",
        "**fashion mnist**  Train Acc: 97.52%   Test Accuracy: 91.93%\n",
        "\n",
        "---\n",
        "\n",
        "**cifar10**: Train Acc: 81.26% Test Accuracy: 78.78%\n",
        "\n",
        "---\n",
        "\n",
        "**svhn**  Train Acc: 96.71% Test Accuracy: 93.17%\n",
        "\n",
        "---\n",
        "\n",
        "**stl** Train Acc: 61.02% Test Accuracy: 47.94%\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**Conv channels [64, 128, 256]**\n",
        "\n",
        "---\n",
        "\n",
        "**mnist** Train Acc: 99.44% Test Accuracy: 99.20%\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**fashion mnist** Train Acc: 94.42% Test Accuracy: 91.91%\n",
        "\n",
        "---\n",
        "\n",
        "**cifar10**: Train Acc: 70.70%  Test Accuracy: 64.46%\n",
        "\n",
        "---\n",
        "\n",
        "**svhn**  Train Acc: 89.96% Test Accuracy: 89.14%\n",
        "\n",
        "---\n",
        "\n",
        "**stl**  Train Acc: 52.23% Test Accuracy: 48.16%\n",
        "\n",
        "---\n",
        "**conv_channels=[32, 64, 128]**\n",
        "\n",
        "**mnist**  Train Acc: 99.49  Test Accuracy: 98.58%\n",
        "\n",
        "---\n",
        "\n",
        "**fashion mnist** Train Acc: 93.30  Test Accuracy: 89.43%\n",
        "\n",
        "---\n",
        "\n",
        "**cifar10**: Train Acc: 68.72%  Test Accuracy: 66.11%\n",
        "\n",
        "---\n",
        "\n",
        "**svhn**  Train Acc: 88.61    Test Accuracy: 91.84%\n",
        "\n",
        "---\n",
        "\n",
        "**stl** Train Acc: 50.33% Test Accuracy: 47.21%\n"
      ],
      "metadata": {
        "id": "KwWsD8JUDcSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Configuration**                | **MNIST (Train / Test)** | **Fashion MNIST (Train / Test)** | **CIFAR-10 (Train / Test)** | **SVHN (Train / Test)** | **STL (Train / Test)** |\n",
        "| -------------------------------- | ------------------------ | -------------------------------- | --------------------------- | ----------------------- | ---------------------- |\n",
        "| **Optimizer: SGD**               | 99.93% / 99.39%          | 97.01% / 89.94%                  | 77.59% / 75.22%             | 96.32% / 91.29%         | 56.38% / 46.46%        |\n",
        "| **Activation: LeakyReLU**        | 99.70% / 99.05%          | 97.64% / 92.29%                  | 81.25% / 75.43%             | 97.10% / 92.46%         | 61.65% / 50.00%        |\n",
        "| **Extra Convolution Layer**      | 99.77% / 98.81%          | 97.52% / 91.93%                  | 81.26% / 78.78%             | 96.71% / 93.17%         | 61.02% / 47.94%        |\n",
        "| **Conv Channels [64, 128, 256]** | 99.44% / 99.20%          | 94.42% / 91.91%                  | 70.70% / 64.46%             | 89.96% / 89.14%         | 52.23% / 48.16%        |\n",
        "| **Conv Channels [32, 64, 128]**  | 99.49% / 98.58%          | 93.30% / 89.43%                  | 68.72% / 66.11%             | 88.61% / 91.84%         | 50.33% / 47.21%        |\n"
      ],
      "metadata": {
        "id": "Rd5A2R4if35i"
      }
    }
  ]
}